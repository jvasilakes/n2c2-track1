task_name: n2c2

# data
train_data: corpus/3/train/
dev_data: corpus/3/dev/
test_data: corpus/3/dev/

# bert
bert_model: bert-base-uncased
#bert_vocab_size: 30522

# vae: bert & gpt2
encoder_model_type: bert
encoder_config_name: bert-base-uncased
encoder_tokenizer_name: bert-base-uncased
encoder_model_name_or_path: bert-base-uncased
#priors_model_name: pre-trained-model/sapBert
#decoder_model_type: gpt2
#decoder_config_name: pre-trained-model/gpt2
#decoder_tokenizer_name: pre-trained-model/gpt2
#decoder_model_name_or_path: pre-trained-model/gpt2

# pre-trained models
# pretrained_vae_global_step: 0
# pre_#ner_model_dir: experiments/3/bert-vae-z0/

# output
result_dir: experiments/3/baseline/
#ner_model_dir: experiments/3/baseline/model/
model_dir: experiments/3/baseline/model/
params_dir: experiments/3/baseline/n2c2.baseline.param

# saving
save_params: True
save_ner: False
save_vae: False
save_all_models: True
save_st_ep: -1

# hyperparameters
epoch: 50
batchsize: 16
ner_learning_rate: 3e-5
#vae_learning_rate: 0.001
#warmup_vae: 10
dropout: 0.3
gpu: 0
seed: 42
fp16: False

# bert
gradient_accumulation_steps: 1
loss_scale: 128
warmup_proportion: 0.1
max_seq: 128
bert_warmup_lr: True

# dimension
bert_dim: 768
hidden_dim: 1000
ner_reduce: False
ner_reduced_size: 500
do_reduce: False

# ner
ner_label_limit: 1
ner_threshold: 0.5
max_entity_width: 14
max_trigger_width: 10

# joint: gold/predicted labels
#use_gold_ner: False
ner_predict_all: True
use_gold_span: False # this is for ner-vae jointly

# joint: joint/pipeline/frozen
skip_ner: False
freeze_bert: False
freeze_ner: False

# joint: starting epoch
ner_epoch: 100
ner_epoch_limit: -1

# joint: loss weight
ner_loss_weight_main: 1
ner_loss_weight_minor: 1

# vae
pretrain_vae: False
use_optimizer: BertAdam
use_bert_vae: False
use_lstm_vae: False
do_lower_case: False
vae_sort_len: False
do_train: False
do_eval: False
global_step_eval: 1000
local_rank: -1
use_philly: False
no_cuda: False
block_size: 1000
latent_size: 768
max_seq_length: 512
eval_batch_size: 1
latent_as_gpt_emb: 1
latent_as_gpt_memory: 1
fb_mode: 1
length_weighted_loss: False
beta: 0.5
bert_#vae_learning_rate: 5e-5
weight_decay: 0.0
adam_epsilon: 1e-8
warmup_steps: 0
max_grad_norm: 1.0
ratio_increase: 0.25
ratio_zero: 0.5
dim_target_kl: 1.0

# joint-vae
vae_joint_ner: False
ner_base: True
ner_vaeBERT: False
ner_vaeBERT_vaeLatent: False
ner_base_vaeLatent: False
vae_feature_based: False
vae_finetune: False
vae_st_ep: 0
vae_loss_scale: 1

#Nhung added
vae_only: False
ner_vae_joint: False 
word_embed_dim: 200
pretrained_embeds_file: pre-trained-model/wordemb/wiki_biowordvec.200d.txt
freeze_words: False
dec_dim: 256
dec_layers: 1
dec_bidirectional: False
teacher_force: 0.3
task_weight: 0.9995
# FLAGS
reconstruction: False
priors: False
use_z: 0
use_pretrain: False
freeze_pretrained: False
show_example: False
sampling_negative: False

# evaluation
ner_eval_corpus: n2c2
rel_eval_script_path: eval_scripts/n2c2.py

# others
direction: l2r+r2l
lab2ign: 1:Other:2
include_nested: True
enable_triggers_pair: True
train: True
rel_lb_weight: -1
lowercase: False
filter_no_ent_sents: False
use_context: True
min_w_freq: 1
unk_w_prob: 0.01
use_lstm: False
stats: True
show_macro: False

# prediction
predict: False
predict_ner: False
t_batch_size: 16
t_gpu: 0
t_fp16: False

